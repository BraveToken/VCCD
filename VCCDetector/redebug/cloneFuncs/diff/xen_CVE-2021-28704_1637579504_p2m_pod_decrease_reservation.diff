--- p2m_pod_decrease_reservation_OLD.c	2021-12-07 00:18:54.404984800 +0800
+++ p2m_pod_decrease_reservation_NEW.c	2021-12-07 00:18:54.404984800 +0800
@@ -1,166 +1,15 @@
 unsigned long
 p2m_pod_decrease_reservation(struct domain *d, gfn_t gfn, unsigned int order)
 {
-    unsigned long ret = 0, i, n;
-    struct p2m_domain *p2m = p2m_get_hostp2m(d);
-    bool_t steal_for_cache;
-    long pod = 0, ram = 0;
+    unsigned long left = 1UL << order, ret = 0;
+    unsigned int chunk_order = find_first_set_bit(gfn_x(gfn) | left);
 
-    gfn_lock(p2m, gfn, order);
-    pod_lock(p2m);
-    p2m->defer_nested_flush = true;
+    do {
+        ret += decrease_reservation(d, gfn, chunk_order);
 
-    /*
-     * If we don't have any outstanding PoD entries, let things take their
-     * course.
-     */
-    if ( p2m->pod.entry_count == 0 )
-        goto out_unlock;
+        left -= 1UL << chunk_order;
+        gfn = gfn_add(gfn, 1UL << chunk_order);
+    } while ( left );
 
-    if ( unlikely(d->is_dying) )
-        goto out_unlock;
-
-    /* Figure out if we need to steal some freed memory for our cache */
-    steal_for_cache =  ( p2m->pod.entry_count > p2m->pod.count );
-
-    for ( i = 0; i < (1UL << order); i += n )
-    {
-        p2m_access_t a;
-        p2m_type_t t;
-        unsigned int cur_order;
-
-        p2m->get_entry(p2m, gfn_add(gfn, i), &t, &a, 0, &cur_order, NULL);
-        n = 1UL << min(order, cur_order);
-        if ( t == p2m_populate_on_demand )
-            pod += n;
-        else if ( p2m_is_ram(t) )
-            ram += n;
-    }
-
-    /* No populate-on-demand?  Don't need to steal anything?  Then we're done!*/
-    if ( !pod && !steal_for_cache )
-        goto out_unlock;
-
-    if ( i == pod )
-    {
-        /*
-         * All PoD: Mark the whole region invalid and tell caller
-         * we're done.
-         */
-        if ( p2m_set_entry(p2m, gfn, INVALID_MFN, order, p2m_invalid,
-                           p2m->default_access) )
-        {
-            /*
-             * If this fails, we can't tell how much of the range was changed.
-             * Best to crash the domain unless we're sure a partial change is
-             * impossible.
-             */
-            if ( order != 0 )
-                domain_crash(d);
-            goto out_unlock;
-        }
-        ret = 1UL << order;
-        p2m->pod.entry_count -= ret;
-        BUG_ON(p2m->pod.entry_count < 0);
-        goto out_entry_check;
-    }
-
-    /*
-     * Try to grab entire superpages if possible.  Since the common case is for
-     * drivers to pass back singleton pages, see if we can take the whole page
-     * back and mark the rest PoD.
-     * No need to do this though if
-     * - order >= SUPERPAGE_ORDER (the loop below will take care of this)
-     * - not all of the pages were RAM (now knowing order < SUPERPAGE_ORDER)
-     */
-    if ( steal_for_cache && order < SUPERPAGE_ORDER && ram == (1UL << order) &&
-         p2m_pod_zero_check_superpage(p2m, _gfn(gfn_x(gfn) & ~(SUPERPAGE_PAGES - 1))) )
-    {
-        pod = 1UL << order;
-        ram = 0;
-        ASSERT(steal_for_cache == (p2m->pod.entry_count > p2m->pod.count));
-    }
-
-    /*
-     * Process as long as:
-     * + There are PoD entries to handle, or
-     * + There is ram left, and we want to steal it
-     */
-    for ( i = 0;
-          i < (1UL << order) && (pod > 0 || (steal_for_cache && ram > 0));
-          i += n )
-    {
-        mfn_t mfn;
-        p2m_type_t t;
-        p2m_access_t a;
-        unsigned int cur_order;
-
-        mfn = p2m->get_entry(p2m, gfn_add(gfn, i), &t, &a, 0, &cur_order, NULL);
-        if ( order < cur_order )
-            cur_order = order;
-        n = 1UL << cur_order;
-        if ( t == p2m_populate_on_demand )
-        {
-            /* This shouldn't be able to fail */
-            if ( p2m_set_entry(p2m, gfn_add(gfn, i), INVALID_MFN, cur_order,
-                               p2m_invalid, p2m->default_access) )
-            {
-                ASSERT_UNREACHABLE();
-                domain_crash(d);
-                goto out_unlock;
-            }
-            p2m->pod.entry_count -= n;
-            BUG_ON(p2m->pod.entry_count < 0);
-            pod -= n;
-            ret += n;
-        }
-        else if ( steal_for_cache && p2m_is_ram(t) )
-        {
-            /*
-             * If we need less than 1 << cur_order, we may end up stealing
-             * more memory here than we actually need. This will be rectified
-             * below, however; and stealing too much and then freeing what we
-             * need may allow us to free smaller pages from the cache, and
-             * avoid breaking up superpages.
-             */
-            struct page_info *page;
-            unsigned long j;
-
-            ASSERT(mfn_valid(mfn));
-
-            page = mfn_to_page(mfn);
-
-            /* This shouldn't be able to fail */
-            if ( p2m_set_entry(p2m, gfn_add(gfn, i), INVALID_MFN, cur_order,
-                               p2m_invalid, p2m->default_access) )
-            {
-                ASSERT_UNREACHABLE();
-                domain_crash(d);
-                goto out_unlock;
-            }
-            p2m_tlb_flush_sync(p2m);
-            for ( j = 0; j < n; ++j )
-                set_gpfn_from_mfn(mfn_x(mfn), INVALID_M2P_ENTRY);
-            p2m_pod_cache_add(p2m, page, cur_order);
-
-            ioreq_request_mapcache_invalidate(d);
-
-            steal_for_cache =  ( p2m->pod.entry_count > p2m->pod.count );
-
-            ram -= n;
-            ret += n;
-        }
-    }
-
-out_entry_check:
-    /* If we've reduced our "liabilities" beyond our "assets", free some */
-    if ( p2m->pod.entry_count < p2m->pod.count )
-    {
-        p2m_pod_set_cache_target(p2m, p2m->pod.entry_count, 0/*can't preempt*/);
-    }
-
-out_unlock:
-    pod_unlock_and_flush(p2m);
-    gfn_unlock(p2m, gfn, order);
     return ret;
 }

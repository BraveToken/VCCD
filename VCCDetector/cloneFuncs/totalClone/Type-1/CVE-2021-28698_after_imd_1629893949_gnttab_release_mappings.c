int gnttab_release_mappings(struct domain *d)
{
    struct grant_table   *gt = d->grant_table, *rgt;
    struct grant_mapping *map;
    grant_ref_t           ref;
    grant_handle_t        handle;
    struct domain        *rd;
    struct active_grant_entry *act;
    grant_entry_header_t *sha;
    uint16_t             *status;
    struct page_info     *pg;

    BUG_ON(!d->is_dying);

    if ( !gt || !gt->maptrack )
        return 0;

    for ( handle = gt->maptrack_limit; handle; )
    {
        mfn_t mfn;

        /*
         * Deal with full pages such that their freeing (in the body of the
         * if()) remains simple.
         */
        if ( handle < gt->maptrack_limit && !(handle % MAPTRACK_PER_PAGE) )
        {
            /*
             * Changing maptrack_limit alters nr_maptrack_frames()'es return
             * value. Free the then excess trailing page right here, rather
             * than leaving it to grant_table_destroy() (and in turn requiring
             * to leave gt->maptrack_limit unaltered).
             */
            gt->maptrack_limit = handle;
            FREE_XENHEAP_PAGE(gt->maptrack[nr_maptrack_frames(gt)]);

            if ( hypercall_preempt_check() )
                return -ERESTART;
        }

        --handle;

        map = &maptrack_entry(gt, handle);
        if ( !(map->flags & (GNTMAP_device_map|GNTMAP_host_map)) )
            continue;

        ref = map->ref;

        gdprintk(XENLOG_INFO, "Grant release %#x ref %#x flags %#x d%d\n",
                 handle, ref, map->flags, map->domid);

        rd = rcu_lock_domain_by_id(map->domid);
        if ( rd == NULL )
        {
            /* Nothing to clear up... */
            map->flags = 0;
            continue;
        }

        rgt = rd->grant_table;
        grant_read_lock(rgt);

        act = active_entry_acquire(rgt, ref);
        sha = shared_entry_header(rgt, ref);
        if ( rgt->gt_version == 1 )
            status = &sha->flags;
        else
            status = &status_entry(rgt, ref);

        pg = mfn_to_page(act->mfn);

        if ( map->flags & GNTMAP_readonly )
        {
            if ( map->flags & GNTMAP_device_map )
            {
                BUG_ON(!(act->pin & GNTPIN_devr_mask));
                act->pin -= GNTPIN_devr_inc;
                if ( !is_iomem_page(act->mfn) )
                    put_page(pg);
            }

            if ( map->flags & GNTMAP_host_map )
            {
                BUG_ON(!(act->pin & GNTPIN_hstr_mask));
                act->pin -= GNTPIN_hstr_inc;
                if ( gnttab_release_host_mappings(d) &&
                     !is_iomem_page(act->mfn) )
                    put_page(pg);
            }
        }
        else
        {
            if ( map->flags & GNTMAP_device_map )
            {
                BUG_ON(!(act->pin & GNTPIN_devw_mask));
                act->pin -= GNTPIN_devw_inc;
                if ( !is_iomem_page(act->mfn) )
                    put_page_and_type(pg);
            }

            if ( map->flags & GNTMAP_host_map )
            {
                BUG_ON(!(act->pin & GNTPIN_hstw_mask));
                act->pin -= GNTPIN_hstw_inc;
                if ( gnttab_release_host_mappings(d) &&
                     !is_iomem_page(act->mfn) )
                {
                    if ( gnttab_host_mapping_get_page_type((map->flags &
                                                            GNTMAP_readonly),
                                                           d, rd) )
                        put_page_type(pg);
                    put_page(pg);
                }
            }
        }

        reduce_status_for_pin(rd, act, status, map->flags & GNTMAP_readonly);

        mfn = act->mfn;

        active_entry_release(act);
        grant_read_unlock(rgt);

        rcu_unlock_domain(rd);

        map->flags = 0;

        /*
         * This is excessive in that a single such call would suffice per
         * mapped MFN (or none at all, if no entry was ever inserted). But it
         * should be the common case for an MFN to be mapped just once, and
         * this way we don't need to further maintain the counters. We also
         * don't want to leave cleaning up of the tree as a whole to the end
         * of the function, as this could take quite some time.
         */
        radix_tree_delete(&gt->maptrack_tree, mfn_x(mfn));
    }

    gt->maptrack_limit = 0;
    FREE_XENHEAP_PAGE(gt->maptrack[0]);

    radix_tree_destroy(&gt->maptrack_tree, NULL);

    return 0;
}

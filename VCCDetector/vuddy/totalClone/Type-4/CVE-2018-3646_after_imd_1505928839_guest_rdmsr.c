int guest_rdmsr(const struct vcpu *v, uint32_t msr, uint64_t *val)
{
    const struct domain *d = v->domain;
    const struct cpuid_policy *cp = d->arch.cpuid;
    const struct msr_policy *mp = d->arch.msr;
    const struct vcpu_msrs *msrs = v->arch.msrs;
    int ret = X86EMUL_OKAY;

    switch ( msr )
    {
    case MSR_AMD_PATCHLOADER:
    case MSR_IA32_UCODE_WRITE:
    case MSR_PRED_CMD:
    case MSR_FLUSH_CMD:
        /* Write-only */
        goto gp_fault;

    case MSR_SPEC_CTRL:
        if ( !cp->feat.ibrsb )
            goto gp_fault;
        *val = msrs->spec_ctrl.raw;
        break;

    case MSR_INTEL_PLATFORM_INFO:
        *val = mp->plaform_info.raw;
        break;

    case MSR_ARCH_CAPABILITIES:
        /* Not implemented yet. */
        goto gp_fault;

    case MSR_INTEL_MISC_FEATURES_ENABLES:
        *val = msrs->misc_features_enables.raw;
        break;

    case 0x40000000 ... 0x400001ff:
        if ( is_viridian_domain(d) )
        {
            ret = (rdmsr_viridian_regs(msr, val)
                   ? X86EMUL_OKAY : X86EMUL_EXCEPTION);
            break;
        }

        /* Fallthrough. */
    case 0x40000200 ... 0x400002ff:
        ret = (rdmsr_hypervisor_regs(msr, val)
               ? X86EMUL_OKAY : X86EMUL_EXCEPTION);
        break;

    default:
        return X86EMUL_UNHANDLEABLE;
    }

    /*
     * Interim safety check that functions we dispatch to don't alias "Not yet
     * handled by the new MSR infrastructure".
     */
    ASSERT(ret != X86EMUL_UNHANDLEABLE);

    return ret;

 gp_fault:
    return X86EMUL_EXCEPTION;
}
